# DataLabeler

## Team Members
- Ahmed Muharram (muharram@seas.upenn.edu)
- Eric Zou (ezou626@seas.upenn.edu)
- Akash Kaukuntla (akashkau@seas.upenn.edu)
- David Zhan (dazhan@sas.upenn.edu)

## Project Resources
- **Video Presentation**: https://www.youtube.com/watch?v=LLBAH43Sk9o
- **Website**: Fully functional application developed
- **Report**: 296042509_DataLabeler.pdf

## Project Synopsis
DataLabeler is a sophisticated application for collecting "dense captions" for images, which can be used to train vision language models. The project represents a fully-featured crowdsourcing platform that tackles the complex problem of generating high-quality training data for AI systems. It positions itself as an open-source alternative to commercial services like Scale AI.

## How It Uses the Crowd
The crowd contributes through:
- **Dense Captioning**: Providing detailed descriptions of images
- **Audio Recording**: Sound recordings associated with images
- **Data Validation**: Quality control through crowd verification
- **Training Data Generation**: Creating datasets for vision-language models

## Strengths
- **Technical Excellence**: Superb implementation with backend database, well-styled frontend, and multiple API integrations
- **Professional Architecture**: Sophisticated system design and execution
- **Clever Aggregation**: Smart use of LLMs for aggregating crowd inputs
- **Strong Quality Control**: Multiple validation mechanisms for text and audio data
- **Well-Thought Ethics**: Comprehensive consideration of ethical implications
- **Real-World Application**: Addresses genuine need for AI training data
- **Potential for Continuation**: Instructor interest in continuing project development

## Weaknesses
- **Presentation Quality**: Video could have been more polished
- **Limited Differentiation**: Doesn't clearly establish unique value beyond existing services
- **Video Content Issues**:
  - Lacked concrete examples of dense captioning in intro
  - Too much focus on MTurk submission mechanics
  - Missing demonstrations of actual task completion
  - Insufficient examples of transcription, aggregation, and QC results

## Course Concept Alignment
The project demonstrates:
- **High understanding** of quality control, aggregation, and ethics
- **Adequate understanding** of crowd dynamics, incentives, skills, and scaling
- Excellent technical implementation aligned with course concepts
- Strong grasp of practical crowdsourcing applications

## Recommendations for Improvement
1. **Enhance Presentation**:
   - Include visual examples of dense captioning
   - Record team members completing tasks
   - Show concrete examples of QC failures and successes
   - Display actual aggregation prompts and results
2. **Clarify Differentiation**:
   - Better articulate advantages over Scale AI
   - Emphasize open-source benefits
   - Highlight unique features
3. **Expand Use Cases**:
   - Consider applications beyond vision-language models
   - Explore self-driving car training data
   - Add support for video captioning
4. **Improve Documentation**:
   - Create comprehensive user guides
   - Document API usage
   - Provide training data examples
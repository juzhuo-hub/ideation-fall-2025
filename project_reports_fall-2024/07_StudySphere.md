# StudySphere

## Team Members
- Elliu Huang (elliuh@seas.upenn.edu)
- Kevin Lu (kevinlu1@seas.upenn.edu)
- Shih Ting Huang (krhuang@seas.upenn.edu)
- Amish Sethi (asethi04@seas.upenn.edu)
- Kerry Zhao (kerrykz@seas.upenn.edu)

## Project Resources
- **Video Presentation**: https://youtu.be/mNZkaV0GZ08
- **Website**: Functional web application with accounts
- **Report**: 296155358_StudySphere.pdf

## Project Synopsis
StudySphere is a platform designed to help students find practice materials for exams and learning. It combines question/answering functionality with flashcard features, similar to Quizlet but with a focus on peer-generated content. Students contribute by submitting study questions and answers, which are then curated through community voting to create high-quality study materials for specific courses.

## How It Uses the Crowd
The crowd contributes through:
- **Question Generation**: Students create study questions for courses
- **Answer Submission**: Multiple answers provided for each question
- **Quality Curation**: Upvoting/downvoting questions and answers
- **Course Coverage**: Questions collected for 9 different classes
- **Peer Review**: Community-driven quality control
- **Difficulty Assessment**: Crowd rates question difficulty levels

## Strengths
- **Social Utility**: Addresses real need for collaborative studying at scale
- **Technical Excellence**: 
  - High-quality implementation with individual accounts
  - Beautiful, coherent UI/UX design
  - Responsive and well-organized frontend
  - Professional backend architecture
- **Good Concept**: Replicates study group experience for remote/large classes
- **Clear Purpose**: Well-defined goal of peer learning support
- **Strong Analysis**: Interesting insights on difficulty preferences (moderate questions most popular)
- **Minimalist Design**: Clean, organized interface

## Weaknesses
- **Weak Incentive Design**: 
  - Only incentive is "pleasure of getting study guide"
  - No mechanism for faculty to track participation
  - Limited motivation for sustained contribution
- **Poor Testing Context**: Tested on NETS 2130 which has no exams
- **Synthetic Environment**: Hard to assess real usefulness without semester-long usage
- **LLM Competition**: Questions can be easily generated by AI, reducing unique value
- **Limited Analytics**: No export mechanism for instructors to evaluate contributions
- **Coverage Issues**: No system to ensure comprehensive course coverage

## Course Concept Alignment
The project demonstrates:
- **High understanding** of crowd dynamics, ethics, quality control, and aggregation
- **Inadequate understanding** of incentive mechanisms
- Good technical execution but weak motivation design
- Strong peer review implementation

## Recommendations for Improvement
1. **Strengthen Incentives**:
   - Add instructor dashboard with contribution metrics
   - Implement participation grades/points
   - Create achievement system
   - Add competitive elements (leaderboards)
2. **Add Faculty Integration**:
   - Export CSV with user metrics (votes given/received)
   - Anonymous identities to prevent gaming
   - Instructor verification of content quality
3. **Enhance Coverage**:
   - Track topic coverage across syllabus
   - Gamify discovery of uncovered topics
   - Show coverage gaps to motivate contributions
4. **Improve Organization**:
   - Categorize questions by chapter/topic
   - Add temporal organization for study schedules
   - Create focused study sessions
5. **Differentiate from AI**:
   - Emphasize human insights and explanations
   - Add personal study tips and tricks
   - Include professor-specific emphasis points
6. **Test in Real Context**:
   - Deploy in courses with actual exams
   - Run semester-long pilot
   - Gather effectiveness data
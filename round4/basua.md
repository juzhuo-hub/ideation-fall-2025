# Round 4 Reflection: Final Project Decision

**Name**: Anika Basu
**PennKey**: 78197908
**Date**: 11/4/2025

---

## 1. What I Explored Today

_List the projects you seriously considered. Keep it brief._

| Project Name | Source | Key Takeaway (1 sentence) |
|--------------|--------|---------------------------|
| PitchPeer | Round 3 | I learned a great possibility for student pitch collaborations, something that's really relevant to students at Penn. Although, I'm not sure whether students would be willing to give as much as they get out of the platform. |
| Kinnect | R2 Drop | This idea seems like a great way for the fitness community to unite and motivate each other. But I'm not sure how much traction it'd be able to gain in the Penn setting, other than maybe Pottruck and athletes. |
| StreetEats | Round 3 | I really like the idea of food truck crowdsourcing as it's something a large crowd can relate to wanting. |

**Resources I used**:
- [ ] Rubric scoring (RUBRIC-PROJECT-VIABILITY.md)
- [ ] V2 detailed analyses (reports/v2-analyses/)
- [x] Steelman Analysis pathways (STEELMAN-ANALYSIS.md)
- [x] Group discussions
- [ ] Other: [specify]

---

## 2. My Decision

**Project Name**: CrowdQA

**Decision type**:
- [ ] STAYING with Round 3 project (same approach)
- [ ] STAYING with Round 3 project (modified approach/scope)
- [X] PIVOTING to different project
- [ ] JOINING another team's project

**If pivoting or adopting someone's idea**:
- Original author (if applicable): Instructor
- Original round: Instructor idea

---

## 3. Why This Decision

**High-level reasoning** (2-3 paragraphs):

Having been a student in lecture many times, I've always wanted something like this but could never put it into words as elegantly as the CrowdQA idea does. Obviously, since Penn is a university there'd be no shortage of lecture halls to try this in. Some members of my planned group are TAs, so actually testing in recitations/large lectures seems feasible.

What draws me the most to this idea is that both parties (students and professors) are helped. Usually a platform is meant to help either students or instructors- there are very few that can actually do both. I think CrowdQA has a strong foundation for helping students express confusion- something they may be embarrassed of doing or think is unimportant- while helping professors understand certain topics they can clarify next time given student confusion stats.

The platform seems fairly simple to implement: just a button. The simpler the implementation, the less room for inaccuracy and concern there is. An easy amount of effort is required on the user's end- simply pushing a button, no required text entry or anything. The convenience and simplicity of using the platform is what makes it seem so great to me.

**What convinced me**:
As mentioned above,
- Plenty of relevant Penn locations to test on (recitations, large lectures, etc.)
- Both students and instructors benefiting from use
- The platform being easy to both implement and eventually use

**What concerns me** (and how I'll address it):
- Since the implementation is so simple, I'm concerned that the project won't be taken seriously or that pitching a simple button to potential users may be difficult. → We'll have to make people understand that the main power of the idea lies in its classroom use. The purpose its meant to serve far outweighs the less implementation effort needed, and that's what maintains the strength of the idea.
- Coming up with variations/ways to upgrade in the future may be hard. It seems like an idea that can plateau. → Between my group, I'm confident that we can find ways to expand, whether that's through testing in different/additional settings, growing the software platform, and/or increasing capabilities.

---

## 4. What I'm Building

**One-sentence project description**:
I plan to build a lecture confusion tracker where during lecture, students anonymously mark confusing moments and after lecture, the instructor sees a real-time heatmap of confusing moments.

**MVP Scope** (3-4 core features only):

1. **Confusion button**: Students pressing this button helps express their confusion during lecture and updates the heatmap.
2. **Lecture session**: Has 1 heatmap corresponding to it containing confusing lecture moments for the professor to analyze. Each new lecture will require to professor to create a new lecture session.
3. **Heatmap**: 1 is created per lecture for the professor to see and understand moments where students got confused, and adjust the next lesson plan to review confusing content.

**What I'm explicitly NOT building** (to keep scope realistic):
We're planning to incorporate all the features mentioned in the original plan, as the platform itself seems relatively straightforward to implement and the features listed all seem necessary for the purpose.

---

## 5. Week 1 Validation

**The specific test I'll run Week 1**:

_Be concrete. Not "social media" but "Post in r/UPenn and 3 class Slacks on Monday at 10am"_

- **Where**: The 4 other lecture classes I have
- **When**: Mon-Thurs with times ranging from 10:15 AM to 8 PM
- **What**: I'll email my professors and ask them if they're interested in trying CrowdQA in their lectures.
- **Success metric**: Hopefully they'll say yes, try it during lecture, and get back to me with how it went, how helpful the students found the confusion button, and how helpful thay found the end-of-lecture heatmap.

**If Week 1 test fails, I will**:
- [X] Pivot to: recitations
- [ ] Use MTurk/paid participants
- [X] Try different recruitment channel: my TA friends' lectures and recitations
- [ ] Simplify the task to: [easier version]
- [ ] Other: [specify]

---

## 6. **Tentative** Team (Optional, Only If You Already Have An Idea)

At this stage, you are not expected to have formed teams, however if you already have an idea of who you intend to work with, you may indicate it here.

**Team members**:

1. Leha Choppara (lehac)
2. Vedha Avali (vavali)
3. Emily Kang (emkang)
4. Amber He (heamber)
5. Mingni Dong (mdong126)

**Team status**:
- [ ] Same team from Round 3
- [X] New team formed during Round 4
- [ ] Solo (will find teammates later)
- [ ] Joining an existing team

---

## 7. Reflection

**Most valuable part of Round 4**:

Hearing that my planned group members explored completely different projects than me. Having them explain the ones they reviewed and explain why they think they're good ideas played a huge role in our final group decision.

**Biggest surprise**:

CrowdQA is such a good idea, I'm surprised I scrolled past it the first time.

**One thing I'd tell future students about Round 4**:

Don't read the same projects as the people sitting with you. Try to have everyone read different projects and convince each other of what's best. I was most convinced when I transitioned to agreeing with someone else after they explained everything rather than agreeing/disagreeing right away because I'd already read that.

---

## Commitment

**I commit to**:
- [X] Building the MVP scope above (3-4 features maximum)
- [X] Running a concrete Week 1 validation test
- [X] Pivoting if Week 1 shows <20% success
- [X] Meeting with instructor if I hit major blockers

**Signature**: Anika Basu **Date**: 11/4/2025

---

## Submission

1. Save as `round4/[your-pennkey].md`
2. Submit via pull request
3. Deadline: [Instructor will specify]

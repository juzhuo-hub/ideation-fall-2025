# Round 4 Reflection: Final Project Decision

**Name**: Nathan Lee
**PennKey**: nzlee
**Date**: 11/4/2025

---

## 1. What I Explored Today

_List the projects you seriously considered. Keep it brief._

| Project Name | Source | Key Takeaway (1 sentence) |
|--------------|--------|---------------------------|
| SoundScape | Round 3  | The SoundScape project is overly ambitious, lacks user demand validation, and has weak incentives, making it unfeasible within 5 weeks. |
| StreetEats | Round 3  | StreetEats addresses a real problem with strong technical design but suffers from a critical cold-start issue and vague recruitment. |
| CrowdQA    |Instructor| CrowdQA is a highly viable and simple solution, with a clear value proposition for both students and instructors, offering immediate impact with a straightforward MVP development timeline. |


**Resources I used**:
- [ ] Rubric scoring (RUBRIC-PROJECT-VIABILITY.md)
- [X] V2 detailed analyses (reports/v2-analyses/)
- [X] Steelman Analysis pathways (STEELMAN-ANALYSIS.md)
- [X] Group discussions
- [ ] Other: [specify]

---

## 2. My Decision

**Project Name**: CrowdQA

**Decision type**:
- [ ] STAYING with Round 3 project (same approach)
- [ ] STAYING with Round 3 project (modified approach/scope)
- [X] PIVOTING to different project
- [ ] JOINING another team's project

**If pivoting or adopting someone's idea**:
- Original author (if applicable): [Name/PennKey]
- Original round: Instructor idea

---

## 3. Why This Decision

**High-level reasoning** (2-3 paragraphs):

I chose CrowdQA because it offers a simple yet highly effective solution to a common problem faced by both students and instructors: understanding when students are confused during lectures. The concept of anonymously tracking moments of confusion during class allows for real-time feedback that can significantly improve teaching effectiveness and enhance student learning experiences. The system is technically feasible with minimal complexity, making it an ideal project for quick implementation. Additionally, since it relies on an existing classroom structure, there’s a built-in user base, and the potential for immediate value creation is clear.

**What convinced me**:
- The technical simplicity of the system allows for rapid development and iteration.
- Clear, immediate value for both students and instructors.
- This would be very helpful as I am a teaching assistant and am always curious if students are confused in my recitations.

**What concerns me** (and how I'll address it):
- Reliance on professor buy-in → I will emphasize the teaching benefits and the minimal effort required from instructors to implement the tool.
- Potential lack of long-term engagement from students → I will design the system to be as seamless and frictionless as possible to encourage regular use during lectures. I could also convince professors to integrate .5 or 1% (a very small fraction) of student's grades to be dependent on using the system as using it benefits both the professors and students.

---

## 4. What I'm Building

**One-sentence project description**:
CrowdQA is a simple web app that allows students to anonymously mark moments of confusion during lectures, providing instructors with a real-time heatmap to improve teaching.
**MVP Scope** (3-4 core features only):


**MVP Scope** (3-4 core features only):

1. **Confusion Button**: A button that students click during the lecture to mark when they are confused, providing instant feedback.
2. **Real-Time Heatmap**: A visual representation of when students are most confused during the lecture, updated in real-time for the instructor.
3. **Simple User Interface**: A clean, easy-to-use web app where students can click the "confused" button with minimal effort.

**What I'm explicitly NOT building** (to keep scope realistic):
- Student tracking or profiling features (e.g., identifying individual students)
- Complex analytics or reports beyond basic heatmaps
- Mobile app support (focus on web platform for MVP)

---

## 5. Week 1 Validation

**The specific test I'll run Week 1**:

_Be concrete. Not "social media" but "Post in r/UPenn and 3 class Slacks on Monday at 10am"_

- **Where**: r/UPenn, 3 class-specific Slack channels
- **When**: Monday at 10am
- **What**: Post a message asking students to try out the CrowdQA system during lectures and provide feedback on ease of use and clarity.
- **Success metric**: At least 20 students click the "confused" button during the lecture and provide feedback in the Slack threads or comments.


**If Week 1 test fails, I will**:
- [X] Pivot to: Increase recruitment by reaching out to other professors for wider class adoption
- [ ] Use MTurk/paid participants
- [X] Try different recruitment channel: Contact class TAs for a direct recommendation to students
- [X] Simplify the task to: Allow students to only mark confusion in terms of topics at the end of the class
- [X] Other: Adjust the messaging to focus more on how the tool benefits instructors

---

## 6. **Tentative** Team (Optional, Only If You Already Have An Idea)

At this stage, you are not expected to have formed teams, however if you already have an idea of who you intend to work with, you may indicate it here.

**Team members**:

1. [Name] ([PennKey]) - [Primary role]
2. [Name] ([PennKey]) - [Primary role]
3. [Name] ([PennKey]) - [Primary role] _(optional)_
4. [Name] ([PennKey]) - [Primary role] _(optional)_

**Team status**:
- [ ] Same team from Round 3
- [ ] New team formed during Round 4
- [X] Solo (will find teammates later)
- [ ] Joining an existing team

---

## 7. Reflection

**Most valuable part of Round 4**:
Reading through the detailed analysis of each proposal's V2 analysis helped clarify the key trade-offs and implementation risks, allowing for a more informed final decision.

**Biggest surprise**:
I was surprised by how often projects seemed overscoped or lacked clear user validation, even when the ideas were strong conceptually.

**One thing I'd tell future students about Round 4**:
Focus on balancing scope, feasibility, and user validation early on—avoid over-promising and under-delivering.
---

## Commitment

**I commit to**:
- [X] Building the MVP scope above (3-4 features maximum)
- [X] Running a concrete Week 1 validation test
- [X] Pivoting if Week 1 shows <20% success
- [X] Meeting with instructor if I hit major blockers

**Signature**: Nathan Lee **Date**: 11/4/2025

---

## Submission

1. Save as `round4/[your-pennkey].md`
2. Submit via pull request
3. Deadline: [Instructor will specify]
